# üß† Human vs AI Text Classification Project

## üìÑ Abstract
With the rapid increase of use of large language models, seperation between human-written and AI-generated text has become a challenging and relevant problem.  
This project focuses on building a model to classify a given text whether generated by AI or by a human.  
Three machine learning models ‚Äî **Multinomial Naive Bayes (MNB)**, **Linear Support Vector Classifier (LinearSVC)**, and a **Feedforward Neural Network (FFN)** ‚Äî were trained and compared.  
An ensemble prediction approach was developed to combine their predictions, and a graphical user interface (GUI) was implemented to visualize results.

---

## üìò Overview
The goal of this project is to classify text samples as either **Human-written** or **AI-generated**.  
Two machine learning models and one deep learning architecture were trained on a balanced subset of a large-scale Kaggle dataset.

A **Tkinter-based GUI** was developed to allow real-time classification.  
For each input text, the ensemble system combines predictions of three models and calculates the probabilities of being *Human* or *AI* and displays them visually.

---

## ‚öôÔ∏è Dataset Information

### üóÇÔ∏è Primary Dataset
- **Source:** [AI vs Human Text Dataset (Kaggle)](https://www.kaggle.com/code/chanchal24/ai-vs-human-text/input)
- **Original Size:** ~487,235 samples
- **Used Subset:** 100,000 samples  
  (50,000 Human-written + 50,000 AI-generated)

> ‚öñÔ∏è **Ethical Note:**  
> Due to the large size of the dataset, only 100,000 samples were used for training and evaluation.  
> The subset was randomly selected while maintaining an equal class distribution to ensure computational feasibility.

---

## üß† Models and Performance Results

### üîπ 1. Multinomial Naive Bayes (MNB)
**Performance Metrics:**
üîµ Accuracy (Train) : 0.946
üü¢ Accuracy (Test) : 0.947
üîµ F1 Score (Train) : 0.945
üü¢ F1 Score (Test) : 0.945

---

### üîπ 2. Linear Support Vector Classifier (LinearSVC)
**Performance Metrics:**
üîµ Accuracy (Train) : 0.999
üü¢ Accuracy (Test) : 0.994
üîµ F1 Score (Train) : 0.999
üü¢ F1 Score (Test) : 0.994

---

### üîπ 3. Feedforward Neural Network (FFN)
**Performance Metrics:**
üîµ Accuracy (Train) : 0.998
üü¢ Accuracy (Test) : 0.994
üîµ F1 Score (Train) : 0.998
üü¢ F1 Score (Test) : 0.994

---

## üß© External Dataset Evaluation

### üìÅ Source:
[Human vs AI Generated Essays (Kaggle)](https://www.kaggle.com/datasets/navjotkaushal/human-vs-ai-generated-essays)

To test models' predictions on unseen data, the trained models were directly evaluated on this external dataset.

**Results:**

| Model           | Test Accuracy |
|-----------------|----------------|
| Linear SVC      | 0.974 |
| Multinomial NB  | 0.990 |
| Deep Learning (FFN) | 1.000 |

As seen above, all models showed superior performance on the unseen dataset. Results showed that three models can be used for ensemble prediction mechanism.

---

## ü§ñ Ensemble Prediction Mechanism
The final system uses an **ensemble strategy** that averages probabilities from all three models to compute the final classification:

The output is displayed in the GUI as:

> üü¢ **Human: X%**  
> üîµ **AI: Y%**

This approach ensures more robust and stable results across different text types, reducing model-specific bias.

---

## ü™ü GUI Application
A **Tkinter-based graphical interface** was implemented to allow users to:
- Enter any text manually.
- Analyze it using the ensemble model.
- Visualize prediction probabilities in real-time via a bar chart.

---

## üßæ Conclusion
- All three models achieved **very high performance (>0.94)**, confirming strong discriminative power between AI-generated and human-written texts.  
- **LinearSVC** and **FFN** achieved top-tier accuracy of **0.994**, while **MNB** performed robustly as a lightweight baseline model (0.947).  
- The **ensemble approach** improved prediction stability and interpretability.
- The GUI makes the model easily accessible for interactive analysis.

---

## üöÄ Future Work
- Integrate explainability tools (e.g., SHAP, LIME) for model interpretation.  
- Improve FFN architecture with dropout and attention layers.  
- Expand GUI with batch prediction, text comparison mode, and result logging.

---

## üß© Project Details

**Author:** Sinan Ustun  
**Date:** 2025  
**Project Type:** Machine Learning / Natural Language Processing (NLP)  
**Language:** Python  
**Frameworks & Libraries:** scikit-learn, TensorFlow/Keras, Tkinter, Matplotlib, Seaborn  
**Datasets:**  
- [AI vs Human Text Dataset](https://www.kaggle.com/code/chanchal24/ai-vs-human-text/input)  
- [Human vs AI Generated Essays](https://www.kaggle.com/datasets/navjotkaushal/human-vs-ai-generated-essays)

---

‚≠ê *This work demonstrates a practical and explainable approach to detecting AI-generated content using classical ML and deep learning models combined with a lightweight, interactive visualization interface.*


